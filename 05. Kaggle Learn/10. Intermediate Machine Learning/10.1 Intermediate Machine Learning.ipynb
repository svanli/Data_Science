{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54d90d12-1edd-4563-a9ff-ed58bd346b81",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <b>Kaggle Learn</b>\n",
    "# 10. Intermediate Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87b07f93-2ff2-4b25-b861-2a6a2e521f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels as sm\n",
    "pd.options.display.max_columns = 20\n",
    "pd.options.display.max_rows = 20\n",
    "pd.options.display.max_colwidth = 80\n",
    "np.set_printoptions(precision = 4, suppress = True)\n",
    "from pandas import Series, DataFrame\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddad8d3-fa2b-4917-a138-f5ba82ef92be",
   "metadata": {},
   "source": [
    "## 2. Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37508d8f-3e5a-4028-8e71-5d5f8a5d9f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('input/melb_data.csv')\n",
    "\n",
    "# Select target\n",
    "y = data['Price']\n",
    "\n",
    "# To keep things simple, we'll use only numerical predictors\n",
    "melb_predictors = data.drop(['Price'], axis = 1)\n",
    "X = melb_predictors.select_dtypes(exclude = ['object'])\n",
    "\n",
    "# Divide data into training and validation subsets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, \n",
    "                                                      train_size = 0.8, \n",
    "                                                      test_size = 0.2,\n",
    "                                                      random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "366e1f26-922f-4dfc-ac48-13b0db66330b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Function for comparing different approaches\n",
    "def score_dataset(X_train, X_valid, y_train, y_valid):\n",
    "    model = RandomForestRegressor(n_estimators = 10, random_state = 0)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_absolute_error(y_valid, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef2c08f9-0fd0-4b4b-9654-cd2650364b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from approach 1 (drop columns with missing values):\n",
      "183550.22137772635\n"
     ]
    }
   ],
   "source": [
    "# Get names of columns with missing values\n",
    "cols_with_missing = [col for col in X_train.columns\n",
    "                     if X_train[col].isnull().any()]\n",
    "\n",
    "# Drop columns in training and validation data\n",
    "reduced_X_train = X_train.drop(cols_with_missing, axis = 1)\n",
    "reduced_X_valid = X_valid.drop(cols_with_missing, axis = 1)\n",
    "\n",
    "print('MAE from approach 1 (drop columns with missing values):')\n",
    "print(score_dataset(reduced_X_train, reduced_X_valid, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ebcb218-f345-42d9-aa6b-197b18cc667e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from approach 2 (imputation):\n",
      "178166.46269899711\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Imputation\n",
    "my_imputer = SimpleImputer()\n",
    "imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))\n",
    "imputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))\n",
    "\n",
    "# Imputation removed column names; put them back\n",
    "imputed_X_train.columns = X_train.columns\n",
    "imputed_X_valid.columns = X_valid.columns\n",
    "\n",
    "print('MAE from approach 2 (imputation):')\n",
    "print(score_dataset(imputed_X_train, imputed_X_valid, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b366fb6a-5ef1-4d92-904e-fb256640c8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from approach 3 (an extension to imputation):\n",
      "178927.503183954\n"
     ]
    }
   ],
   "source": [
    "# Make a copy to avoid changing original data (when imputing)\n",
    "X_train_plus = X_train.copy()\n",
    "X_valid_plus = X_valid.copy()\n",
    "\n",
    "# Make new columns indicating what will be imputed\n",
    "for col in cols_with_missing:\n",
    "    X_train_plus[col + '_was_missing'] = X_train_plus[col].isnull()\n",
    "    X_valid_plus[col + '_was_missing'] = X_valid_plus[col].isnull()\n",
    "    \n",
    "# Imputation\n",
    "my_imputer = SimpleImputer()\n",
    "imputed_X_train_plus = pd.DataFrame(my_imputer.fit_transform(X_train_plus))\n",
    "imputed_X_valid_plus = pd.DataFrame(my_imputer.transform(X_valid_plus))\n",
    "\n",
    "# Imputation removed column names; put them back\n",
    "imputed_X_train_plus.columns = X_train_plus.columns\n",
    "imputed_X_valid_plus.columns = X_valid_plus.columns\n",
    "\n",
    "print('MAE from approach 3 (an extension to imputation):')\n",
    "print(score_dataset(imputed_X_train_plus, imputed_X_valid_plus, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a53aedd-3932-4c4f-be17-abdea1b4f59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10864, 12)\n",
      "Car               49\n",
      "BuildingArea    5156\n",
      "YearBuilt       4307\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Shape of training data (num_rows, num_columns)\n",
    "print(X_train.shape)\n",
    "\n",
    "# Number of missing values in each column of training data\n",
    "missing_val_count_by_column = X_train.isnull().sum()\n",
    "print(missing_val_count_by_column[missing_val_count_by_column > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00146d4a-b84a-44c2-8203-f57e8609c3f8",
   "metadata": {},
   "source": [
    "## 3. Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30ce2d30-220f-4e47-ae45-629255f88517",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the data\n",
    "data = pd.read_csv('input/melb_data.csv')\n",
    "\n",
    "# Separate target from predictors\n",
    "y = data['Price']\n",
    "X = data.drop(['Price'], axis=1)\n",
    "\n",
    "# Divide data into training and validation subsets\n",
    "X_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y,\n",
    "                                                                train_size = 0.8,\n",
    "                                                                test_size = 0.2,\n",
    "                                                                random_state = 0)\n",
    "\n",
    "# Drop columns with missing values (simplest approach)\n",
    "cols_with_missing = [col for col in X_train_full.columns if X_train_full[col].isnull().any()] \n",
    "X_train_full.drop(cols_with_missing, axis = 1, inplace = True)\n",
    "X_valid_full.drop(cols_with_missing, axis = 1, inplace = True)\n",
    "\n",
    "# \"Cardinality\" means the number of unique values in a column\n",
    "# Select categorical columns with relatively low cardinality (convenient but arbitrary)\n",
    "low_cardinality_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and \n",
    "                        X_train_full[cname].dtype == \"object\"]\n",
    "\n",
    "# Select numerical columns\n",
    "numerical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n",
    "\n",
    "# Keep selected columns only\n",
    "my_cols = low_cardinality_cols + numerical_cols\n",
    "X_train = X_train_full[my_cols].copy()\n",
    "X_valid = X_valid_full[my_cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "843ab004-3ea3-49ef-a3b4-94709b53ba1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Method</th>\n",
       "      <th>Regionname</th>\n",
       "      <th>Rooms</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Postcode</th>\n",
       "      <th>Bedroom2</th>\n",
       "      <th>Bathroom</th>\n",
       "      <th>Landsize</th>\n",
       "      <th>Lattitude</th>\n",
       "      <th>Longtitude</th>\n",
       "      <th>Propertycount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12167</th>\n",
       "      <td>u</td>\n",
       "      <td>S</td>\n",
       "      <td>Southern Metropolitan</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3182.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-37.85984</td>\n",
       "      <td>144.9867</td>\n",
       "      <td>13240.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6524</th>\n",
       "      <td>h</td>\n",
       "      <td>SA</td>\n",
       "      <td>Western Metropolitan</td>\n",
       "      <td>2</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3016.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>-37.85800</td>\n",
       "      <td>144.9005</td>\n",
       "      <td>6380.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8413</th>\n",
       "      <td>h</td>\n",
       "      <td>S</td>\n",
       "      <td>Western Metropolitan</td>\n",
       "      <td>3</td>\n",
       "      <td>12.6</td>\n",
       "      <td>3020.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>555.0</td>\n",
       "      <td>-37.79880</td>\n",
       "      <td>144.8220</td>\n",
       "      <td>3755.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2919</th>\n",
       "      <td>u</td>\n",
       "      <td>SP</td>\n",
       "      <td>Northern Metropolitan</td>\n",
       "      <td>3</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3046.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>265.0</td>\n",
       "      <td>-37.70830</td>\n",
       "      <td>144.9158</td>\n",
       "      <td>8870.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6043</th>\n",
       "      <td>h</td>\n",
       "      <td>S</td>\n",
       "      <td>Western Metropolitan</td>\n",
       "      <td>3</td>\n",
       "      <td>13.3</td>\n",
       "      <td>3020.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>673.0</td>\n",
       "      <td>-37.76230</td>\n",
       "      <td>144.8272</td>\n",
       "      <td>4217.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Type Method             Regionname  Rooms  Distance  Postcode  Bedroom2  \\\n",
       "12167    u      S  Southern Metropolitan      1       5.0    3182.0       1.0   \n",
       "6524     h     SA   Western Metropolitan      2       8.0    3016.0       2.0   \n",
       "8413     h      S   Western Metropolitan      3      12.6    3020.0       3.0   \n",
       "2919     u     SP  Northern Metropolitan      3      13.0    3046.0       3.0   \n",
       "6043     h      S   Western Metropolitan      3      13.3    3020.0       3.0   \n",
       "\n",
       "       Bathroom  Landsize  Lattitude  Longtitude  Propertycount  \n",
       "12167       1.0       0.0  -37.85984    144.9867        13240.0  \n",
       "6524        2.0     193.0  -37.85800    144.9005         6380.0  \n",
       "8413        1.0     555.0  -37.79880    144.8220         3755.0  \n",
       "2919        1.0     265.0  -37.70830    144.9158         8870.0  \n",
       "6043        1.0     673.0  -37.76230    144.8272         4217.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c738b18c-3ad6-4080-808f-80d4b577abaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical variables:\n",
      "['Type', 'Method', 'Regionname']\n"
     ]
    }
   ],
   "source": [
    "# Get list of categorical vairables\n",
    "s = (X_train.dtypes == 'object')\n",
    "object_cols = list(s[s].index)\n",
    "\n",
    "print(\"Categorical variables:\")\n",
    "print(object_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3513a1b2-d186-42ba-9285-dd6d14810614",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Function for comparing different approaches\n",
    "def score_dataset(X_train, X_valid, y_train, y_valid):\n",
    "    model = RandomForestRegressor(n_estimators = 100, random_state = 0)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_absolute_error(y_valid, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6416216-8128-47a8-872a-f83f2b8690fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from approach 1 (drop categorical variables):\n",
      "175703.48185157913\n"
     ]
    }
   ],
   "source": [
    "drop_X_train = X_train.select_dtypes(exclude = ['object'])\n",
    "drop_X_valid = X_valid.select_dtypes(exclude = ['object'])\n",
    "\n",
    "print(\"MAE from approach 1 (drop categorical variables):\")\n",
    "print(score_dataset(drop_X_train, drop_X_valid, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d94456f7-c1c3-4449-89aa-57beb04e3797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from approach 2 (ordinal encoding):\n",
      "165936.40548390493\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Make copy to avoid changing original data\n",
    "label_X_train = X_train.copy()\n",
    "label_X_valid = X_valid.copy()\n",
    "\n",
    "# Apply ordinal encoder to each column with categorical data\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "label_X_train[object_cols] = ordinal_encoder.fit_transform(X_train[object_cols])\n",
    "label_X_valid[object_cols] = ordinal_encoder.transform(X_valid[object_cols])\n",
    "\n",
    "print(\"MAE from approach 2 (ordinal encoding):\")\n",
    "print(score_dataset(label_X_train, label_X_valid, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b16ffb5-c9f5-4f1b-ab2a-53085dbd26db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from approach 3 (one-hot encoding):\n",
      "166089.4893009678\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Apply one-hot encoder to each column with categorical data\n",
    "# [handle_unknown = 'ignore']: avoid errors when the validation data contains classes\n",
    "# that aren't represented in the training data\n",
    "# [sparse = False]: ensure that the encoded columns are returned as a numpy array \n",
    "# (instead of a sparse matrix)\n",
    "OH_encoder = OneHotEncoder(handle_unknown = 'ignore', sparse = False)\n",
    "OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))\n",
    "OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[object_cols]))\n",
    "\n",
    "# One-hot encoding removed index; put it back\n",
    "OH_cols_train.index = X_train.index\n",
    "OH_cols_valid.index = X_valid.index\n",
    "\n",
    "# Remove categorical columns (will replace with one-hot encoding)\n",
    "num_X_train = X_train.drop(object_cols, axis = 1)\n",
    "num_X_valid = X_valid.drop(object_cols, axis = 1)\n",
    "\n",
    "# Add one-hot encoded columns to numerical features\n",
    "OH_X_train = pd.concat([num_X_train, OH_cols_train], axis = 1)\n",
    "OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis = 1)\n",
    "\n",
    "# Ensure all columns have string type\n",
    "OH_X_train.columns = OH_X_train.columns.astype(str)\n",
    "OH_X_valid.columns = OH_X_valid.columns.astype(str)\n",
    "\n",
    "print(\"MAE from approach 3 (one-hot encoding):\")\n",
    "print(score_dataset(OH_X_train, OH_X_valid, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06bf337-93d7-428e-9ff0-7cba5a6e66f5",
   "metadata": {},
   "source": [
    "## 4. Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "500e5d34-a8e4-46da-81cc-351ce3e0fc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the data\n",
    "data = pd.read_csv('input/melb_data.csv')\n",
    "\n",
    "# Separate target from predictors\n",
    "y = data.Price\n",
    "X = data.drop(['Price'], axis = 1)\n",
    "\n",
    "# Divide data into training and validation subsets\n",
    "X_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y,\n",
    "                                                                train_size = 0.8,\n",
    "                                                                test_size = 0.2,\n",
    "                                                                random_state = 0)\n",
    "\n",
    "# \"Cardinality\" means the number of unique values in a column\n",
    "# Select categorical columns with relatively low cardinality (convenient but arbitrary)\n",
    "categorical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and \n",
    "                        X_train_full[cname].dtype == \"object\"]\n",
    "\n",
    "# Select numerical columns\n",
    "numerical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n",
    "\n",
    "# Keep selected columns only\n",
    "my_cols = categorical_cols + numerical_cols\n",
    "X_train = X_train_full[my_cols].copy()\n",
    "X_valid = X_valid_full[my_cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80522750-cf51-465e-992a-25f10e8279c4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset:\n",
      "   Age  Income\n",
      "0   25   50000\n",
      "1   35   60000\n",
      "2   28   48000\n",
      "3   31   70000\n",
      "4   22   45000\n",
      "5   27   52000\n",
      "6   32   75000\n",
      "7   29   49000\n",
      "8   26   51000\n",
      "9   30   68000\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "data = {\n",
    "    'Age': [25, 35, 28, 31, 22, 27, 32, 29, 26, 30],\n",
    "    'Income': [50000, 60000, 48000, 70000, 45000, 52000, 75000, 49000, 51000, 68000]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Original Dataset:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d737636-509d-41a4-ab96-cdba38288863",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtered Dataset Before Splitting:\n",
      "   Age  Income\n",
      "0   25   50000\n",
      "2   28   48000\n",
      "4   22   45000\n",
      "5   27   52000\n",
      "7   29   49000\n",
      "8   26   51000\n",
      "\n",
      "Training Data Before Splitting:\n",
      "   Age  Income\n",
      "2   28   48000\n",
      "5   27   52000\n",
      "0   25   50000\n",
      "7   29   49000\n",
      "\n",
      "Testing Data Before Splitting:\n",
      "   Age  Income\n",
      "8   26   51000\n",
      "4   22   45000\n"
     ]
    }
   ],
   "source": [
    "# Filter before split\n",
    "filtered_df_before = df[df['Age'] < 30]\n",
    "\n",
    "# Perform train/test split\n",
    "train_df_before, test_df_before = train_test_split(filtered_df_before, test_size=0.2, random_state=0)\n",
    "\n",
    "print(\"\\nFiltered Dataset Before Splitting:\")\n",
    "print(filtered_df_before)\n",
    "print(\"\\nTraining Data Before Splitting:\")\n",
    "print(train_df_before)\n",
    "print(\"\\nTesting Data Before Splitting:\")\n",
    "print(test_df_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf06c850-d91f-4fab-997a-3f05fb2a76ab",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Data After Splitting:\n",
      "   Age  Income\n",
      "4   22   45000\n",
      "7   29   49000\n",
      "0   25   50000\n",
      "5   27   52000\n",
      "\n",
      "Testing Data After Splitting:\n",
      "   Age  Income\n",
      "2   28   48000\n",
      "8   26   51000\n"
     ]
    }
   ],
   "source": [
    "# Perform train/test split first\n",
    "train_df_after, test_df_after = train_test_split(df, test_size=0.2, random_state=0)\n",
    "\n",
    "# Filter after split\n",
    "train_df_after = train_df_after[train_df_after['Age'] < 30]\n",
    "test_df_after = test_df_after[test_df_after['Age'] < 30]\n",
    "\n",
    "print(\"\\nTraining Data After Splitting:\")\n",
    "print(train_df_after)\n",
    "print(\"\\nTesting Data After Splitting:\")\n",
    "print(test_df_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c25bf933-0a0c-4952-99c8-c35c1961a49c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset:\n",
      "   Feature  Leakage  Target\n",
      "0       10      100       0\n",
      "1       20      200       1\n",
      "2       30      300       0\n",
      "3       40      400       1\n",
      "4       50      500       0\n",
      "5       60      600       1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create a synthetic dataset\n",
    "data = {\n",
    "    'Feature': [10, 20, 30, 40, 50, 60],\n",
    "    'Leakage': [100, 200, 300, 400, 500, 600],\n",
    "    'Target': [0, 1, 0, 1, 0, 1]  # Binary target (0 or 1)\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the original dataset\n",
    "print(\"Original Dataset:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4ae0440-ac14-4e78-89ae-42347669f2ad",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Accuracy (with data leakage): 0.50\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing step before splitting (data leakage)\n",
    "scaler = StandardScaler()\n",
    "df[['Feature', 'Leakage']] = scaler.fit_transform(df[['Feature', 'Leakage']])\n",
    "\n",
    "# Perform train/test split after preprocessing (incorrect order)\n",
    "X = df[['Feature', 'Leakage']]  # Features\n",
    "y = df['Target']  # Target variable\n",
    "\n",
    "# Incorrectly split data after preprocessing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a model (e.g., Logistic Regression)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "accuracy = model.score(X_test, y_test)\n",
    "print(f\"\\nModel Accuracy (with data leakage): {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "368f1068-0cac-487a-9077-b4887d1a5447",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Accuracy (without data leakage): 0.50\n"
     ]
    }
   ],
   "source": [
    "# Correct order: Split data first, then preprocess\n",
    "X = df[['Feature', 'Leakage']]  # Features\n",
    "y = df['Target']  # Target variable\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess the training data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # Fit scaler on training data only\n",
    "\n",
    "# Train a model using the preprocessed training data\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Preprocess the test data using the same scaler\n",
    "X_test_scaled = scaler.transform(X_test)  # Use scaler fitted on training data for test data\n",
    "\n",
    "# Evaluate the model on the preprocessed test data\n",
    "accuracy = model.score(X_test_scaled, y_test)\n",
    "print(f\"\\nModel Accuracy (without data leakage): {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "79369213-d5fb-4cac-a427-aee72be92655",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Dataset:\n",
      "   Feature  Leakage  Target\n",
      "0       10      100       0\n",
      "1       20      200       1\n",
      "2       30      300       0\n",
      "3       40      400       1\n",
      "4       50      500       1\n",
      "5       60      600       1\n",
      "\n",
      "Model Accuracy (with data leakage): 0.50\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create a synthetic dataset with a meaningful relationship\n",
    "data = {\n",
    "    'Feature': [10, 20, 30, 40, 50, 60],\n",
    "    'Leakage': [100, 200, 300, 400, 500, 600],\n",
    "    'Target': [0, 1, 0, 1, 1, 1]  # Updated target variable\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the updated dataset\n",
    "print(\"Updated Dataset:\")\n",
    "print(df)\n",
    "\n",
    "# Preprocessing step before splitting (data leakage)\n",
    "scaler = StandardScaler()\n",
    "df[['Feature', 'Leakage']] = scaler.fit_transform(df[['Feature', 'Leakage']])\n",
    "\n",
    "# Perform train/test split after preprocessing (incorrect order)\n",
    "X = df[['Feature', 'Leakage']]  # Features\n",
    "y = df['Target']  # Target variable\n",
    "\n",
    "# Incorrectly split data after preprocessing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a model (e.g., Logistic Regression)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "accuracy = model.score(X_test, y_test)\n",
    "print(f\"\\nModel Accuracy (with data leakage): {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d6ca65a3-741c-41f4-9418-8fdf3c6f6dad",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Accuracy (without data leakage): 0.50\n"
     ]
    }
   ],
   "source": [
    "# Correct order: Split data first, then preprocess\n",
    "X = df[['Feature', 'Leakage']]  # Features\n",
    "y = df['Target']  # Target variable\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess the training data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # Fit scaler on training data only\n",
    "\n",
    "# Train a model using the preprocessed training data\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Preprocess the test data using the same scaler\n",
    "X_test_scaled = scaler.transform(X_test)  # Use scaler fitted on training data for test data\n",
    "\n",
    "# Evaluate the model on the preprocessed test data\n",
    "accuracy = model.score(X_test_scaled, y_test)\n",
    "print(f\"\\nModel Accuracy (without data leakage): {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca70b836-d31e-4b6c-9613-7493d4b5583f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Dataset:\n",
      "   Feature  Leakage  Target\n",
      "0       10        0       0\n",
      "1       20        1       1\n",
      "2       30        0       0\n",
      "3       40        1       1\n",
      "4       50        1       0\n",
      "5       60        1       1\n",
      "\n",
      "Model Accuracy (with data leakage): 0.50\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create a synthetic dataset with a meaningful relationship and leakage\n",
    "data = {\n",
    "    'Feature': [10, 20, 30, 40, 50, 60],\n",
    "    'Leakage': [0, 1, 0, 1, 1, 1],  # Direct correlation with Target\n",
    "    'Target': [0, 1, 0, 1, 0, 1]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the updated dataset\n",
    "print(\"Updated Dataset:\")\n",
    "print(df)\n",
    "\n",
    "# Preprocessing step before splitting (data leakage)\n",
    "scaler = StandardScaler()\n",
    "df[['Feature', 'Leakage']] = scaler.fit_transform(df[['Feature', 'Leakage']])\n",
    "\n",
    "# Perform train/test split after preprocessing (incorrect order)\n",
    "X = df[['Feature', 'Leakage']]  # Features including leakage\n",
    "y = df['Target']  # Target variable\n",
    "\n",
    "# Incorrectly split data after preprocessing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a model (e.g., Logistic Regression)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "accuracy = model.score(X_test, y_test)\n",
    "print(f\"\\nModel Accuracy (with data leakage): {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4e7aff2b-d1e5-4312-8699-d791344c4305",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Dataset:\n",
      "   Feature  Leakage  Target\n",
      "0       10        0       0\n",
      "1       20        1       1\n",
      "2       30        0       0\n",
      "3       40        1       1\n",
      "4       50        1       0\n",
      "5       60        1       1\n",
      "\n",
      "Model Accuracy (with data leakage): 0.50\n",
      "\n",
      "Model Accuracy (without data leakage): 0.50\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create a synthetic dataset with a clear data leakage scenario\n",
    "data = {\n",
    "    'Feature': [10, 20, 30, 40, 50, 60],\n",
    "    'Leakage': [0, 1, 0, 1, 1, 1],  # Direct correlation with Target\n",
    "    'Target': [0, 1, 0, 1, 0, 1]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the updated dataset\n",
    "print(\"Updated Dataset:\")\n",
    "print(df)\n",
    "\n",
    "# Incorrect approach: Preprocessing before splitting (data leakage)\n",
    "scaler = StandardScaler()\n",
    "df[['Feature', 'Leakage']] = scaler.fit_transform(df[['Feature', 'Leakage']])\n",
    "\n",
    "# Perform train/test split after preprocessing (incorrect order)\n",
    "X = df[['Feature', 'Leakage']]  # Features including leakage\n",
    "y = df['Target']  # Target variable\n",
    "\n",
    "# Incorrectly split data after preprocessing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a model (e.g., Logistic Regression) on preprocessed data (incorrectly)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set (with data leakage)\n",
    "accuracy_leakage = model.score(X_test, y_test)\n",
    "print(f\"\\nModel Accuracy (with data leakage): {accuracy_leakage:.2f}\")\n",
    "\n",
    "# Correct approach: Split data first, then preprocess to avoid data leakage\n",
    "X = df[['Feature', 'Leakage']]  # Features including leakage\n",
    "y = df['Target']  # Target variable\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess the training data only (without leakage)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Train a model on preprocessed training data (correct approach)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Preprocess the test data using the same scaler\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Evaluate the model on the preprocessed test data (without data leakage)\n",
    "accuracy_no_leakage = model.score(X_test_scaled, y_test)\n",
    "print(f\"\\nModel Accuracy (without data leakage): {accuracy_no_leakage:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "49eb12ae-021e-49ea-a2be-6affd645b9fb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset:\n",
      "   Feature  Target\n",
      "0       10       0\n",
      "1       20       1\n",
      "2       30       0\n",
      "3       40       1\n",
      "4       50       0\n",
      "5       60       1\n",
      "\n",
      "Model Accuracy (with data leakage): 0.50\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create a synthetic dataset with data leakage scenario\n",
    "data = {\n",
    "    'Feature': [10, 20, 30, 40, 50, 60],\n",
    "    'Target': [0, 1, 0, 1, 0, 1]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the dataset\n",
    "print(\"Original Dataset:\")\n",
    "print(df)\n",
    "\n",
    "# Preprocessing before splitting (data leakage)\n",
    "scaler = StandardScaler()\n",
    "df['Feature_scaled'] = scaler.fit_transform(df[['Feature']])\n",
    "\n",
    "# Perform train/test split after preprocessing (incorrect order)\n",
    "X = df[['Feature_scaled']]  # Scaled feature\n",
    "y = df['Target']  # Target variable\n",
    "\n",
    "# Incorrectly split data after preprocessing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a model on preprocessed data (incorrectly)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set (with data leakage)\n",
    "accuracy_leakage = model.score(X_test, y_test)\n",
    "print(f\"\\nModel Accuracy (with data leakage): {accuracy_leakage:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4699aee5-9808-4130-a2ae-a66c40db8a1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Method</th>\n",
       "      <th>Regionname</th>\n",
       "      <th>Rooms</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Postcode</th>\n",
       "      <th>Bedroom2</th>\n",
       "      <th>Bathroom</th>\n",
       "      <th>Car</th>\n",
       "      <th>Landsize</th>\n",
       "      <th>BuildingArea</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>Lattitude</th>\n",
       "      <th>Longtitude</th>\n",
       "      <th>Propertycount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12167</th>\n",
       "      <td>u</td>\n",
       "      <td>S</td>\n",
       "      <td>Southern Metropolitan</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3182.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1940.0</td>\n",
       "      <td>-37.85984</td>\n",
       "      <td>144.9867</td>\n",
       "      <td>13240.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6524</th>\n",
       "      <td>h</td>\n",
       "      <td>SA</td>\n",
       "      <td>Western Metropolitan</td>\n",
       "      <td>2</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3016.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-37.85800</td>\n",
       "      <td>144.9005</td>\n",
       "      <td>6380.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8413</th>\n",
       "      <td>h</td>\n",
       "      <td>S</td>\n",
       "      <td>Western Metropolitan</td>\n",
       "      <td>3</td>\n",
       "      <td>12.6</td>\n",
       "      <td>3020.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>555.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-37.79880</td>\n",
       "      <td>144.8220</td>\n",
       "      <td>3755.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2919</th>\n",
       "      <td>u</td>\n",
       "      <td>SP</td>\n",
       "      <td>Northern Metropolitan</td>\n",
       "      <td>3</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3046.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>265.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>-37.70830</td>\n",
       "      <td>144.9158</td>\n",
       "      <td>8870.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6043</th>\n",
       "      <td>h</td>\n",
       "      <td>S</td>\n",
       "      <td>Western Metropolitan</td>\n",
       "      <td>3</td>\n",
       "      <td>13.3</td>\n",
       "      <td>3020.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>673.0</td>\n",
       "      <td>673.0</td>\n",
       "      <td>1970.0</td>\n",
       "      <td>-37.76230</td>\n",
       "      <td>144.8272</td>\n",
       "      <td>4217.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Type Method             Regionname  Rooms  Distance  Postcode  Bedroom2  \\\n",
       "12167    u      S  Southern Metropolitan      1       5.0    3182.0       1.0   \n",
       "6524     h     SA   Western Metropolitan      2       8.0    3016.0       2.0   \n",
       "8413     h      S   Western Metropolitan      3      12.6    3020.0       3.0   \n",
       "2919     u     SP  Northern Metropolitan      3      13.0    3046.0       3.0   \n",
       "6043     h      S   Western Metropolitan      3      13.3    3020.0       3.0   \n",
       "\n",
       "       Bathroom  Car  Landsize  BuildingArea  YearBuilt  Lattitude  \\\n",
       "12167       1.0  1.0       0.0           NaN     1940.0  -37.85984   \n",
       "6524        2.0  1.0     193.0           NaN        NaN  -37.85800   \n",
       "8413        1.0  1.0     555.0           NaN        NaN  -37.79880   \n",
       "2919        1.0  1.0     265.0           NaN     1995.0  -37.70830   \n",
       "6043        1.0  2.0     673.0         673.0     1970.0  -37.76230   \n",
       "\n",
       "       Longtitude  Propertycount  \n",
       "12167    144.9867        13240.0  \n",
       "6524     144.9005         6380.0  \n",
       "8413     144.8220         3755.0  \n",
       "2919     144.9158         8870.0  \n",
       "6043     144.8272         4217.0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "adbfebd4-2f4d-4003-89dd-0f0a5fc3438f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Preprocessing for numerical data\n",
    "numerical_transformer = SimpleImputer(strategy = 'constant')\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "categorical_transformer = Pipeline(steps = [\n",
    "    ('imputer', SimpleImputer(strategy = 'most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown = 'ignore'))\n",
    "])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers = [\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "64f505ae-99f9-430b-ba53-602360ab9f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor(n_estimators = 100, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "89977334-3adf-456d-97b6-00f2b11462c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 160679.18917034855\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Bundle preprocessing and modeling code in a pipeline\n",
    "my_pipeline = Pipeline(steps = [('preprocessor', preprocessor),\n",
    "                                ('model', model)\n",
    "                               ])\n",
    "\n",
    "# Preprocessing of training data, fit model\n",
    "my_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Preprocessing of validation data, get predictions\n",
    "preds = my_pipeline.predict(X_valid)\n",
    "\n",
    "# Evaluate the model\n",
    "score = mean_absolute_error(y_valid, preds)\n",
    "print('MAE:', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a419778-5757-49b8-8ecc-60192081fbfe",
   "metadata": {},
   "source": [
    "## 5. Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e520ce2-92eb-4f00-93f4-73692b4821ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the data\n",
    "data = pd.read_csv('input/melb_data.csv')\n",
    "\n",
    "# Select subset of predictors\n",
    "cols_to_use = ['Rooms', 'Distance', 'Landsize', 'BuildingArea', 'YearBuilt']\n",
    "X = data[cols_to_use]\n",
    "\n",
    "# Select target\n",
    "y = data['Price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fa430fc-32d6-4f5e-9602-1c1e536045cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "my_pipeline = Pipeline(steps = [('preprocessor', SimpleImputer()),\n",
    "                                ('model', RandomForestRegressor(n_estimators = 50,\n",
    "                                                                random_state = 0))\n",
    "                               ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c64a9099-52e0-4693-91f3-9f2cfb5f1e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE scores:\n",
      " [301628.7894 303164.4783 287298.3317 236061.8475 260383.4511]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Multiply by -1 since sklearn calculates *negative* MAE\n",
    "scores = -1 * cross_val_score(my_pipeline, X, y,\n",
    "                              cv = 5,\n",
    "                              scoring = 'neg_mean_absolute_error')\n",
    "\n",
    "print('MAE scores:\\n', scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea67490d-56d8-4315-8685-8c09880e9b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MAE score (across experiments):\n",
      "277707.3795913405\n"
     ]
    }
   ],
   "source": [
    "print('Average MAE score (across experiments):')\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87ae6f0-e71f-4abe-aaf3-0b465b069166",
   "metadata": {},
   "source": [
    "## 6. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a93e07c-8917-4123-96fb-54f9853293b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the data\n",
    "data = pd.read_csv('input/melb_data.csv')\n",
    "\n",
    "# Select subset of predictors\n",
    "cols_to_use = ['Rooms', 'Distance', 'Landsize', 'BuildingArea', 'YearBuilt']\n",
    "X = data[cols_to_use]\n",
    "\n",
    "# Select target\n",
    "y = data['Price']\n",
    "\n",
    "# Separate data into training and validation sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51e107e3-a7c7-4b19-96e6-73b6019512b6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-2.0.3-py3-none-win_amd64.whl (99.8 MB)\n",
      "     ---------------------------------------- 99.8/99.8 MB 2.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from xgboost) (1.21.5)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from xgboost) (1.9.1)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-2.0.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbce446d-b66c-4db7-9175-d14aee924c48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "             num_parallel_tree=None, random_state=None, ...)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "my_model = XGBRegressor()\n",
    "my_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f8bd77e-b4e5-451e-aa68-08beea891cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 244967.28017765094\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "predictions = my_model.predict(X_valid)\n",
    "print('Mean Absolute Error: ' + str(mean_absolute_error(predictions, y_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f59aa7c-fff5-484d-a924-9e2c30cd1b26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=500, n_jobs=None,\n",
       "             num_parallel_tree=None, random_state=None, ...)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model = XGBRegressor(n_estimators = 500)\n",
    "my_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e477f53-b807-4b8c-bb06-e10dd32796e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=500, n_jobs=None,\n",
       "             num_parallel_tree=None, random_state=None, ...)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model = XGBRegressor(n_estimators = 500)\n",
    "my_model.fit(X_train, y_train,\n",
    "             early_stopping_rounds = 5,\n",
    "             eval_set = [(X_valid, y_valid)],\n",
    "             verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0054e27-3968-4e18-bdd0-8e84fdd398b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopped at learning_rate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
