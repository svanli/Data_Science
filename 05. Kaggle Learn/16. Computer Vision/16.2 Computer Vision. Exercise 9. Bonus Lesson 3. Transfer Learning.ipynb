{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":24196,"sourceType":"datasetVersion","datasetId":4427},{"sourceId":872462,"sourceType":"datasetVersion","datasetId":464180}],"dockerImageVersionId":30646,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Transfer Learning\n\nIf you took the [Computer Vision course](https://www.kaggle.com/learn/computer-vision), you learned how to use a model to make predictions. But what if you have a new use-case, and you don’t categorize images in exactly the same way as the categories for your original model?\n\nFor example, I might want a model that can tell if a photo was taken in an urban area or a rural area, but my original model doesn’t classify images into those two specific categories.  I could build a new model from scratch for this specific purpose. But to get good results, I’d need thousands of photos with labels for which are urban and which are rural.\n\nA method called transfer learning will give good results with far less data. Transfer learning takes what a model learned while solving one problem (called a **pre-trained model**, because the model has already been trained on a different dataset), and applies it to a new application.\n\n# Goal of this Notebook\n\n[ImageNet](https://en.wikipedia.org/wiki/ImageNet) is a very large image dataset, composed of over 14 million images from thousands of categories.  Keras makes available several models that have been pre-trained on this dataset [here](https://keras.io/api/applications/).  One of the models is [ResNet](https://keras.io/api/applications/resnet/#resnet50-function).\n\nIn this notebook, we'll show you how to adapt the pre-trained ResNet model to a new task to predict if an image is rural or urban.  You'll work with [this dataset](https://www.kaggle.com/datasets/dansbecker/urban-and-rural-photos).\n\n# Background\n\nRemember that early layers of a deep learning model identify simple shapes. Later layers identify more complex visual patterns, like roads, buildings, windows, and open fields. These layers will be useful in our new application.  \n\n<img src=\"https://i.imgur.com/NdcEZhf.png\" width=500px>\n\nThe very last layer makes predictions.  We’ll drop in a replacement for this last layer of the ResNet model. \n\nThe replacement is a dense layer with two nodes. One node captures how urban the photo is, and the other captures how rural it is. In theory, any node in the last layer before prediction might inform how urban it is. So the urban measure can depend on all the nodes in this layer.  We draw connections to show that possible relationship.  For the same reason, the information at each node might affect our measure of how rural the photo is.\n\n<img src=\"https://i.imgur.com/ZsJWiDV.png\" width=350px>\n\n\nWe have a lot of connections here, and we’ll use training data to determine which nodes suggest an image is urban, which suggest it is rural, and which don’t matter.  That is, we’re going to be training the last layer of the model. In practice, that training data will be photos that are labeled as either rural or urban.  \n\nNote: When classifying something into only 2 categories, we could get by with only one node at the output.  In this case, a prediction for how urban a photo is would also be a measure of how rural it is.  If a photo is 80% likely to be urban, it would be 20% likely to be rural. But we’ve kept two separate nodes at the output layer.  Using a separate node for each possible category in the output layer will help us transition to cases when we want to predict with more than 2 categories.","metadata":{}},{"cell_type":"markdown","source":"# Code\n\n### Specify the Model\n\nIn this application, we classify photos into 2 categories or classes, urban and rural. We’ll save that as `num_classes`.\n\nNext we build the model. We set up a sequential model that we can add layers to. First we add a pre-trained ResNet model. When creating the ResNet model, we’ve written `include_top=False`. This is how we specify that we want to exclude the last layer of the ResNet model that makes predictions.  We’ll also use a file that doesn’t include the weights for that layer.\n\nThe argument `pooling='avg'` says that if we had extra channels in our tensor at the end of this step, we want to collapse them to a 1D tensor by taking an average.  Now we have a pretrained model that creates the layer you saw in the graphic.  We’ll add a `Dense` layer to make predictions.  We specify the number of nodes in this layer, which in this case is the number of classes. Then we apply the softmax function to produce probabilities.\n\n<img src=\"https://i.imgur.com/RutdkVs.png\" width=500px>\n\nFinally, we’ll tell TensorFlow not to train the first layer of the sequential model, the ResNet50 layers. This is because that’s the model that was already pre-trained with the ImageNet data. \n","metadata":{}},{"cell_type":"code","source":"# set random seed / make reproducible\nimport random\nimport numpy as np\nimport tensorflow as tf\nseed = 0\nrandom.seed(seed)\nnp.random.seed(seed)\ntf.random.set_seed(seed)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-05-30T18:12:00.645980Z","iopub.execute_input":"2024-05-30T18:12:00.646379Z","iopub.status.idle":"2024-05-30T18:12:14.878710Z","shell.execute_reply.started":"2024-05-30T18:12:00.646349Z","shell.execute_reply":"2024-05-30T18:12:14.877754Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-05-30 18:12:02.671958: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-30 18:12:02.672078: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-30 18:12:02.818752: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"from tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense\n\nnum_classes = 2\nresnet_weights_path = '../input/resnet50/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\n\nmy_new_model = Sequential()\nmy_new_model.add(ResNet50(include_top=False, pooling='avg', weights=resnet_weights_path))\nmy_new_model.add(Dense(num_classes, activation='softmax'))\n\n# Say not to train first layer (ResNet) model. It is already trained\nmy_new_model.layers[0].trainable = False\n\nmy_new_model.summary()","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2024-05-30T18:12:14.880534Z","iopub.execute_input":"2024-05-30T18:12:14.881137Z","iopub.status.idle":"2024-05-30T18:12:19.578297Z","shell.execute_reply.started":"2024-05-30T18:12:14.881105Z","shell.execute_reply":"2024-05-30T18:12:19.577208Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Model: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n resnet50 (Functional)       (None, 2048)              23587712  \n                                                                 \n dense (Dense)               (None, 2)                 4098      \n                                                                 \n=================================================================\nTotal params: 23591810 (90.00 MB)\nTrainable params: 4098 (16.01 KB)\nNon-trainable params: 23587712 (89.98 MB)\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Compile the Model\n\nThe compile command tells TensorFlow how to update the relationships in the final layer of the network during training.\n\nWe have a measure of loss or inaccuracy we want to minimize. We specify it as `categorical_crossentropy`. If you are familiar with log-loss, this is another term for the same thing.\n\nWe use an algorithm called stochastic gradient descent (SGD) to minimize the categorical cross-entropy loss. \n\nWe ask the code to report the accuracy metric, the fraction of correct predictions. This is easier to interpret than categorical cross-entropy scores, so it’s nice to print it out and see how the model is doing.","metadata":{}},{"cell_type":"code","source":"my_new_model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-05-30T18:12:40.550348Z","iopub.execute_input":"2024-05-30T18:12:40.551301Z","iopub.status.idle":"2024-05-30T18:12:40.567959Z","shell.execute_reply.started":"2024-05-30T18:12:40.551265Z","shell.execute_reply":"2024-05-30T18:12:40.566722Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### Load the Image Data\n\nOur raw data is broken into a directory of training data and a directory of validation data. Within each of those, we have one subdirectory for the urban pictures and another for the rural pictures. TensorFlow provides a great tool for working with images grouped into directories by their label.  This is the `ImageDataGenerator`. \n\nThere are two steps to using `ImageDataGenerator`. First we create the generator object in the abstract. We want to apply the ResNet preprocessing function every time it reads in an image. \n\nThen we use the `flow_from_directory` command. We tell it what directory that data is in, what size image we want, how many images to read in at a time (the batch size), and we tell it we’re classifying data into different categories. We do the same thing to set up a way to read the validation data.\n\n`ImageDataGenerator` is especially very valuable when working with large datasets, because we don’t need to hold the whole dataset in memory at once. But it’s also nice here, with a small dataset. Note that these are generators which means we need to iterate over them to get data out.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.applications.resnet50 import preprocess_input\nfrom tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n\nimage_size = 224\ndata_generator = ImageDataGenerator(preprocessing_function=preprocess_input)\n\n\ntrain_generator = data_generator.flow_from_directory(\n        '../input/urban-and-rural-photos/train',\n        target_size=(image_size, image_size),\n        batch_size=12,\n        class_mode='categorical')\n\nvalidation_generator = data_generator.flow_from_directory(\n        '../input/urban-and-rural-photos/val',\n        target_size=(image_size, image_size),\n        batch_size=20,\n        class_mode='categorical')","metadata":{"execution":{"iopub.status.busy":"2024-05-30T18:12:41.403425Z","iopub.execute_input":"2024-05-30T18:12:41.404120Z","iopub.status.idle":"2024-05-30T18:12:41.983409Z","shell.execute_reply.started":"2024-05-30T18:12:41.404086Z","shell.execute_reply":"2024-05-30T18:12:41.982001Z"},"trusted":true},"execution_count":5,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresnet50\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m preprocess_input\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageDataGenerator\n\u001b[1;32m      4\u001b[0m image_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m224\u001b[39m\n\u001b[1;32m      5\u001b[0m data_generator \u001b[38;5;241m=\u001b[39m ImageDataGenerator(preprocessing_function\u001b[38;5;241m=\u001b[39mpreprocess_input)\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.python.keras.preprocessing'"],"ename":"ModuleNotFoundError","evalue":"No module named 'tensorflow.python.keras.preprocessing'","output_type":"error"}]},{"cell_type":"markdown","source":"### Fit the Model\n\nNow we fit the model.  The training data comes from `train_generator`, and the validation data comes from `validation_generator`.  Since we have 72 training images and read in 12 images at a time, we use 6 steps for a single epoch (`steps_per_epoch=6`).  Likewise, we have 20 validation images, and use one validation step since we read in all 20 images in a single step (`validation_steps=1`).\n\nAs the model training is running, we’ll see progress updates showing with our loss function and the accuracy. It updates the connections in the dense layer, that is the model’s impression of what makes an urban photo and what makes a rural photo. When it’s done, it gets 78% of the training data right.  Then it examines the validation data. It gets 90% of those right. \n\nI should mention that this is a really small dataset and you should be hesitant about relying on validation scores from such a small amount of data.  We’re starting with small datasets so you can get some experience under your belt with models that can be trained quickly.","metadata":{}},{"cell_type":"code","source":"my_new_model.fit(\n        train_generator,\n        steps_per_epoch=6,\n        validation_data=validation_generator,\n        validation_steps=1)","metadata":{"execution":{"iopub.status.busy":"2024-05-30T18:12:47.056715Z","iopub.execute_input":"2024-05-30T18:12:47.057112Z","iopub.status.idle":"2024-05-30T18:12:47.092944Z","shell.execute_reply.started":"2024-05-30T18:12:47.057084Z","shell.execute_reply":"2024-05-30T18:12:47.091149Z"},"trusted":true},"execution_count":6,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m my_new_model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m----> 2\u001b[0m         \u001b[43mtrain_generator\u001b[49m,\n\u001b[1;32m      3\u001b[0m         steps_per_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m,\n\u001b[1;32m      4\u001b[0m         validation_data\u001b[38;5;241m=\u001b[39mvalidation_generator,\n\u001b[1;32m      5\u001b[0m         validation_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'train_generator' is not defined"],"ename":"NameError","evalue":"name 'train_generator' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"Even with the small training dataset, this accuracy score is really good. We trained on 72 photos. You could easily take that many photos on your phone, upload them to [Kaggle Datasets](https://www.kaggle.com/datasets), and build a very accurate model to distinguish almost anything you care about. \n\n### Note on Results\nThe printed validation accuracy can be meaningfully better than the training accuracy at this stage. This can be puzzling at first.\n\nIt occurs because the training accuracy was calculated at multiple points as the network was improving (the numbers in the convolutions were being updated to make the model more accurate).  The network was inaccurate when the model saw the first training images, since the weights hadn't been trained/improved much yet.  Those first training results were averaged into the measure above.\n\nThe validation loss and accuracy measures were calculated **after** the model had gone through all the data.  So the network had been fully trained when these scores were calculated.\n\nThis isn't a serious issue in practice, and we tend not to worry about it.","metadata":{}}]}